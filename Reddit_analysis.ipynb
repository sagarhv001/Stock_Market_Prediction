{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "thAL50MXayKq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "from datetime import datetime\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rolling_window = 5\n",
        "commentDF = pd.read_csv('/content/comment_analysis_Indian_Stock_Market_NSE.csv')\n",
        "commentDF['Rolling_Vader_Pos'] = commentDF['Vader Pos'].rolling(window=rolling_window).mean()\n",
        "commentDF['Rolling_Vader_Neg'] = commentDF['Vader Neg'].rolling(window=rolling_window).mean()\n",
        "commentDF['Rolling_TextBlob_Pos'] = commentDF['textblob Positive'].rolling(window=rolling_window).mean()\n",
        "commentDF['Rolling_TextBlob_Neg'] = commentDF['textblob Negative'].rolling(window=rolling_window).mean()\n",
        "\n",
        "# Fill NA values resulting from rolling calculation\n",
        "commentDF.fillna(0, inplace=True)"
      ],
      "metadata": {
        "id": "px70PO83hkDy"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "commentDF['Weighted_Vader_Pos'] = commentDF['Vader Pos'] * commentDF['NumberOfTickerMentions']\n",
        "commentDF['Weighted_Vader_Neg'] = commentDF['Vader Neg'] * commentDF['NumberOfTickerMentions']\n",
        "commentDF['Weighted_TextBlob_Pos'] = commentDF['textblob Positive'] * commentDF['NumberOfTickerMentions']\n",
        "commentDF['Weighted_TextBlob_Neg'] = commentDF['textblob Negative'] * commentDF['NumberOfTickerMentions']\n",
        "\n",
        "# Optional: Normalize by dividing by the total mentions in the same window\n",
        "commentDF['Normalized_Weighted_Vader_Pos'] = commentDF['Weighted_Vader_Pos'] / (commentDF['NumberOfTickerMentions'] + 1e-5)\n",
        "commentDF['Normalized_Weighted_Vader_Neg'] = commentDF['Weighted_Vader_Neg'] / (commentDF['NumberOfTickerMentions'] + 1e-5)"
      ],
      "metadata": {
        "id": "d1NDV3B4-b83"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "commentDF['Combined_Sentiment_Pos'] = commentDF['Rolling_Vader_Pos'] + commentDF['Normalized_Weighted_Vader_Pos']\n",
        "commentDF['Combined_Sentiment_Neg'] = commentDF['Rolling_Vader_Neg'] + commentDF['Normalized_Weighted_Vader_Neg']\n"
      ],
      "metadata": {
        "id": "VU9eJPaM-e8i"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "commentDF.to_csv('comment_analysis_with_sentiments.csv', index=False)"
      ],
      "metadata": {
        "id": "buSrKnKu-hv-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comment_df = pd.read_csv('comment_analysis_with_sentiments.csv')\n",
        "stock_df = pd.read_csv('/content/stockhistory_ZOMATO.NS.csv')"
      ],
      "metadata": {
        "id": "45mYlTaf-lgK"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Comment Data Columns:\", comment_df.columns)\n",
        "print(\"Stock Data Columns:\", stock_df.columns)\n",
        "comment_df['Date'] = pd.to_datetime(comment_df['Date'], errors='coerce')\n",
        "stock_df['Date'] = pd.to_datetime(stock_df['Date'], errors='coerce')\n",
        "print(\"Missing values in comment_df 'Date':\", comment_df['Date'].isnull().sum())\n",
        "print(\"Missing values in stock_df 'Date':\", stock_df['Date'].isnull().sum())\n",
        "stock_df = stock_df.dropna(subset=['Date'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERtuRDhG-o5T",
        "outputId": "0c800e11-4d64-458f-b074-80e7e1fe2d48"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comment Data Columns: Index(['Title', 'Ticker', 'Date', 'NumberOfTickerMentions', 'Vader Neg',\n",
            "       'Vader Pos', 'Vader Neut', 'textblob Negative', 'textblob Positive',\n",
            "       'textblob Neut', 'Rolling_Vader_Pos', 'Rolling_Vader_Neg',\n",
            "       'Rolling_TextBlob_Pos', 'Rolling_TextBlob_Neg', 'Weighted_Vader_Pos',\n",
            "       'Weighted_Vader_Neg', 'Weighted_TextBlob_Pos', 'Weighted_TextBlob_Neg',\n",
            "       'Normalized_Weighted_Vader_Pos', 'Normalized_Weighted_Vader_Neg',\n",
            "       'Combined_Sentiment_Pos', 'Combined_Sentiment_Neg'],\n",
            "      dtype='object')\n",
            "Stock Data Columns: Index(['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Dividends',\n",
            "       'Stock Splits'],\n",
            "      dtype='object')\n",
            "Missing values in comment_df 'Date': 0\n",
            "Missing values in stock_df 'Date': 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "invalid_dates = comment_df[~comment_df['Date'].apply(pd.to_datetime, errors='coerce').notna()]\n",
        "print(invalid_dates)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W581c6ww-r7N",
        "outputId": "bae683d9-43ab-40c4-9333-4a4a024659dc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [Title, Ticker, Date, NumberOfTickerMentions, Vader Neg, Vader Pos, Vader Neut, textblob Negative, textblob Positive, textblob Neut, Rolling_Vader_Pos, Rolling_Vader_Neg, Rolling_TextBlob_Pos, Rolling_TextBlob_Neg, Weighted_Vader_Pos, Weighted_Vader_Neg, Weighted_TextBlob_Pos, Weighted_TextBlob_Neg, Normalized_Weighted_Vader_Pos, Normalized_Weighted_Vader_Neg, Combined_Sentiment_Pos, Combined_Sentiment_Neg]\n",
            "Index: []\n",
            "\n",
            "[0 rows x 22 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from datetime import datetime\n",
        "import re\n",
        "\n",
        "# Define function to check if the comment contains the stock ticker\n",
        "def contains_ticker(comment, ticker_keywords):\n",
        "    # Regex to match ticker symbol in the comment based on ticker_keywords\n",
        "    pattern = rf\"\\b(?:{'|'.join(ticker_keywords)})\\b\"\n",
        "    return bool(re.search(pattern, comment, re.IGNORECASE))\n",
        "\n",
        "# Filter comments by the ticker keywords\n",
        "def filter_comments_by_ticker(comment_df, ticker_keywords):\n",
        "    # Use boolean indexing to filter the DataFrame directly for keywords\n",
        "    relevant_comments = comment_df[comment_df['Title'].str.contains(rf\"\\b(?:{'|'.join(ticker_keywords)})\\b\", case=False, na=False)]\n",
        "    return relevant_comments\n",
        "\n",
        "# Merge the comment dataframe with stock data based on Date\n",
        "def merge_sentiment_and_stock_data(comment_df, stock_df):\n",
        "    # Ensure 'Date' columns in both dataframes are datetime objects\n",
        "    # Convert Date columns to datetime in both dataframes\n",
        "    comment_df['Date'] = pd.to_datetime(comment_df['Date'], errors='coerce').dt.tz_localize(None)\n",
        "    stock_df = stock_df.reset_index()\n",
        "    stock_df['Date'] = pd.to_datetime(stock_df['Date'], errors='coerce').dt.tz_localize(None)\n",
        "    # Merge filtered comments with stock price data (using 'Date')\n",
        "    final_df = pd.merge(comment_df, stock_df, on='Date', how='inner')\n",
        "    return final_df\n",
        "\n",
        "# Function to process and merge the data\n",
        "def process_data():\n",
        "    # Define the stock ticker and keywords you're interested in\n",
        "    ticker = 'ZOMATO.NS'  # Replace with the correct ticker format\n",
        "    ticker_keywords = ['ZOMATO']  # List of keywords to match in comments\n",
        "\n",
        "    # Load your comment data\n",
        "    comment_df = pd.read_csv('comment_analysis_with_sentiments.csv')\n",
        "\n",
        "    # Load stock price data (e.g., from Yahoo Finance for ticker 'ADANIENT.NSE')\n",
        "    # Get stock data for the last 6 months\n",
        "\n",
        "    if stock_df.empty:\n",
        "        print(f\"No stock data available for {ticker}. Please check the ticker symbol or try another period.\")\n",
        "        return\n",
        "\n",
        "    # Filter comments by the target ticker keywords\n",
        "    relevant_comment_df = filter_comments_by_ticker(comment_df, ticker_keywords)\n",
        "\n",
        "    # Check if relevant_comment_df is empty\n",
        "    if relevant_comment_df.empty:\n",
        "        print(f\"No comments found for ticker: {ticker}\")\n",
        "        return\n",
        "\n",
        "    # Merge the filtered comment data with stock price data based on the 'Date' column\n",
        "    final_df = merge_sentiment_and_stock_data(relevant_comment_df, stock_df)\n",
        "\n",
        "    # Save the merged dataframe to a CSV file\n",
        "    final_df.to_csv(f'merged_data_{ticker}.csv', index=False)\n",
        "\n",
        "    # Display the merged dataframe\n",
        "    print(final_df)\n",
        "\n",
        "# Run the process_data function\n",
        "process_data()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEVEPKuH-vGg",
        "outputId": "34536c9b-fb50-4975-e898-8ac8079c64ca"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               Title Ticker       Date  \\\n",
            "0                          Zomato doubled my money 🥹    NSE 2023-11-08   \n",
            "1             Zomato- Blinkit - Things don't add up.    NSE 2024-09-27   \n",
            "2                             Zomato Investors today    NSE 2022-08-02   \n",
            "3  Why market fell today? Why is Zomato falling? ...    NSE 2024-05-27   \n",
            "4  Zomato, swiggy hike platform fee by 20% to Rs ...    NSE 2024-07-15   \n",
            "5       What do you think will happen to Zomato now?    NSE 2022-08-22   \n",
            "6  Sell shovels during a Gold rush. Pudumjee Pape...    NSE 2024-12-02   \n",
            "\n",
            "   NumberOfTickerMentions  Vader Neg  Vader Pos  Vader Neut  \\\n",
            "0                       1         12         39          47   \n",
            "1                      32          9         21          33   \n",
            "2                      47          8          8          22   \n",
            "3                      68          5         15          34   \n",
            "4                      74         16         38          54   \n",
            "5                      81         14         14          44   \n",
            "6                      81          3         15          15   \n",
            "\n",
            "   textblob Negative  textblob Positive  textblob Neut  ...  \\\n",
            "0                 15                 36             47  ...   \n",
            "1                 23                 29             11  ...   \n",
            "2                  8                 15             15  ...   \n",
            "3                  9                 24             21  ...   \n",
            "4                 18                 54             36  ...   \n",
            "5                 19                 21             32  ...   \n",
            "6                  6                 15             12  ...   \n",
            "\n",
            "   Combined_Sentiment_Pos  Combined_Sentiment_Neg  index        Open  \\\n",
            "0               82.599610               27.399880    569  122.000000   \n",
            "1               38.999993               15.399997    786  275.399994   \n",
            "2               36.799998               24.599998    256   50.000000   \n",
            "3               32.999998               10.599999    700  183.899994   \n",
            "4               61.599995               27.399998    734  225.000000   \n",
            "5               37.999998               26.999998    268   61.000000   \n",
            "6               23.399998                8.600000    829  281.899994   \n",
            "\n",
            "         High         Low       Close     Volume  Dividends  Stock Splits  \n",
            "0  125.699997  121.300003  125.150002   75041688        0.0           0.0  \n",
            "1  286.899994  273.500000  278.149994   58812631        0.0           0.0  \n",
            "2   55.549999   48.400002   55.549999  528899748        0.0           0.0  \n",
            "3  185.449997  178.399994  183.649994   47652253        0.0           0.0  \n",
            "4  232.000000  225.000000  229.149994   51203051        0.0           0.0  \n",
            "5   62.849998   59.250000   62.000000  226773007        0.0           0.0  \n",
            "6  285.700012  280.450012  282.500000   35170688        0.0           0.0  \n",
            "\n",
            "[7 rows x 30 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-1e1f86cde112>:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  comment_df['Date'] = pd.to_datetime(comment_df['Date'], errors='coerce').dt.tz_localize(None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f9UrnH3Y_2e2"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "\n",
        "def merge_all_csvs(output_file, sort_by=\"Date\"):\n",
        "    \"\"\"\n",
        "    Merges all CSV files matching 'merged_data_*.csv' into a single file and sorts by date.\n",
        "\n",
        "    Args:\n",
        "        output_file (str): Path to save the final merged and sorted CSV file.\n",
        "        sort_by (str): Column to sort by. Default is 'Date'.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Get all merged_data_*.csv files\n",
        "        csv_files = glob.glob(\"/content/merged_data_*.csv\")\n",
        "\n",
        "        if not csv_files:\n",
        "            print(\"No CSV files found matching 'merged_data_*.csv'\")\n",
        "            return\n",
        "\n",
        "        # Read and combine all CSV files\n",
        "        combined_df = pd.concat([pd.read_csv(file) for file in csv_files], ignore_index=True)\n",
        "\n",
        "        # Ensure the sort column is in datetime format if it's a date\n",
        "        if sort_by in combined_df.columns:\n",
        "            combined_df[sort_by] = pd.to_datetime(combined_df[sort_by], errors='coerce')\n",
        "\n",
        "        # Sort the combined DataFrame by the Date column\n",
        "        sorted_df = combined_df.sort_values(by=sort_by)\n",
        "        sorted_df.drop_duplicates(inplace=True)\n",
        "\n",
        "        # Save the merged and sorted DataFrame to a new CSV\n",
        "        sorted_df.to_csv(output_file, index=False)\n",
        "        print(f\"All CSVs have been merged and saved to: {output_file}\")\n",
        "        print(sorted_df.head())  # Print first few rows for verification\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "# Example usage\n",
        "merge_all_csvs(\"merged_data_all.csv\", sort_by=\"Date\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnMsiz0_GhO2",
        "outputId": "04d5e4cf-2f11-447f-d05b-d3dd4ed4e71b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All CSVs have been merged and saved to: merged_data_all.csv\n",
            "                                               Title Ticker       Date  \\\n",
            "2                             Zomato Investors today    NSE 2022-08-02   \n",
            "5       What do you think will happen to Zomato now?    NSE 2022-08-22   \n",
            "0                          Zomato doubled my money 🥹    NSE 2023-11-08   \n",
            "3  Why market fell today? Why is Zomato falling? ...    NSE 2024-05-27   \n",
            "4  Zomato, swiggy hike platform fee by 20% to Rs ...    NSE 2024-07-15   \n",
            "\n",
            "   NumberOfTickerMentions  Vader Neg  Vader Pos  Vader Neut  \\\n",
            "2                      47          8          8          22   \n",
            "5                      81         14         14          44   \n",
            "0                       1         12         39          47   \n",
            "3                      68          5         15          34   \n",
            "4                      74         16         38          54   \n",
            "\n",
            "   textblob Negative  textblob Positive  textblob Neut  ...  \\\n",
            "2                  8                 15             15  ...   \n",
            "5                 19                 21             32  ...   \n",
            "0                 15                 36             47  ...   \n",
            "3                  9                 24             21  ...   \n",
            "4                 18                 54             36  ...   \n",
            "\n",
            "   Combined_Sentiment_Pos  Combined_Sentiment_Neg  index        Open  \\\n",
            "2               36.799998               24.599998    256   50.000000   \n",
            "5               37.999998               26.999998    268   61.000000   \n",
            "0               82.599610               27.399880    569  122.000000   \n",
            "3               32.999998               10.599999    700  183.899994   \n",
            "4               61.599995               27.399998    734  225.000000   \n",
            "\n",
            "         High         Low       Close     Volume  Dividends  Stock Splits  \n",
            "2   55.549999   48.400002   55.549999  528899748        0.0           0.0  \n",
            "5   62.849998   59.250000   62.000000  226773007        0.0           0.0  \n",
            "0  125.699997  121.300003  125.150002   75041688        0.0           0.0  \n",
            "3  185.449997  178.399994  183.649994   47652253        0.0           0.0  \n",
            "4  232.000000  225.000000  229.149994   51203051        0.0           0.0  \n",
            "\n",
            "[5 rows x 30 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the merged data\n",
        "merged_data = pd.read_csv('merged_data_all.csv')\n",
        "\n",
        "# Load the original comment data\n",
        "comment_data = pd.read_csv('comment_analysis_with_sentiments.csv')\n",
        "\n",
        "# Find comments not present in the merged data\n",
        "# Assuming 'Title' is the unique identifier in both DataFrames\n",
        "comments_not_in_merged = comment_data[~comment_data['Title'].isin(merged_data['Title'])]\n",
        "\n",
        "# Save the comments not in merged data to a new CSV file\n",
        "comments_not_in_merged.to_csv('general.csv', index=False)"
      ],
      "metadata": {
        "id": "3_KdM493BI4X"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For the General Comments\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from datetime import datetime\n",
        "\n",
        "# Merge the comment dataframe with stock data based on Date\n",
        "def merge_sentiment_and_stock_data(comment_df, stock_df):\n",
        "    # Ensure 'Date' columns in both dataframes are datetime objects\n",
        "    # Convert Date columns to datetime in both dataframes\n",
        "    comment_df['Date'] = pd.to_datetime(comment_df['Date'], errors='coerce').dt.tz_localize(None)\n",
        "    stock_df = stock_df.reset_index()\n",
        "    stock_df['Date'] = pd.to_datetime(stock_df['Date'], errors='coerce').dt.tz_localize(None)\n",
        "    # Merge filtered comments with stock price data (using 'Date')\n",
        "    final_df = pd.merge(comment_df, stock_df, on='Date', how='inner')\n",
        "    return final_df\n",
        "\n",
        "# Function to process and merge the data\n",
        "def process_data():\n",
        "    # Define the stock ticker for NIFTY 50\n",
        "    ticker = '^NSEI'  # Ticker for NIFTY 50\n",
        "\n",
        "    # Load your comment data\n",
        "    comment_df = pd.read_csv('general.csv')\n",
        "\n",
        "    # Fetch stock price data from Yahoo Finance\n",
        "    # Last 1 year of data\n",
        "\n",
        "    # Check if stock data is empty\n",
        "    if stock_df.empty:\n",
        "        print(f\"No stock data available for {ticker}. Please check the ticker symbol or try another period.\")\n",
        "        return\n",
        "\n",
        "    # Merge the comment data with stock price data based on the 'Date' column\n",
        "    final_df = merge_sentiment_and_stock_data(comment_df, stock_df)\n",
        "\n",
        "    # Save the merged dataframe to a CSV file\n",
        "    output_file = f'merged_data_{ticker}.csv'\n",
        "    final_df.to_csv(output_file, index=False)\n",
        "\n",
        "    # Display the merged dataframe\n",
        "    print(f\"Merged data saved to {output_file}\")\n",
        "    print(final_df)\n",
        "\n",
        "# Run the process_data function\n",
        "process_data()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbOYa-tXJMhA",
        "outputId": "e3bc0417-4fa7-4578-99b5-c9170d178209"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged data saved to merged_data_^NSEI.csv\n",
            "                                                 Title Ticker       Date  \\\n",
            "0                                            Thoughts?    NSE 2023-10-30   \n",
            "1    You win some, you lose some. When you do win, ...    NSE 2023-11-30   \n",
            "2                    Should I buy and hold for a year?    NSE 2023-09-14   \n",
            "3    For newbies here, -10% is correction and -30% ...    NSE 2024-08-05   \n",
            "4    I Lost My Friend to F&O Trading: Please Don’t ...    NSE 2024-10-21   \n",
            "..                                                 ...    ...        ...   \n",
            "682  Somebody knew about ADANI on 19th Itself,,,,,,...    NSE 2024-11-21   \n",
            "683                                    Exit poll fraud    NSE 2024-06-04   \n",
            "684                                  Today’s Headlines    NSE 2024-04-12   \n",
            "685                Experts ne toh value trap bola tha😯    NSE 2023-09-18   \n",
            "686                           Koi nazar nahi lagayega     NSE 2024-12-05   \n",
            "\n",
            "     NumberOfTickerMentions  Vader Neg  Vader Pos  Vader Neut  \\\n",
            "0                         1         76        140         264   \n",
            "1                         1         12         68          71   \n",
            "2                         1         21         14          53   \n",
            "3                         1         28         26          72   \n",
            "4                         1         31         23          70   \n",
            "..                      ...        ...        ...         ...   \n",
            "682                      84          9         10          23   \n",
            "683                      84         10         21          37   \n",
            "684                      84          0          6           2   \n",
            "685                      84          1          2           7   \n",
            "686                      84         15         21          88   \n",
            "\n",
            "     textblob Negative  textblob Positive  textblob Neut  ...  \\\n",
            "0                   73                218            189  ...   \n",
            "1                   14                 86             51  ...   \n",
            "2                   17                 19             52  ...   \n",
            "3                   16                 49             61  ...   \n",
            "4                   39                 55             30  ...   \n",
            "..                 ...                ...            ...  ...   \n",
            "682                  7                 17             18  ...   \n",
            "683                 17                 29             22  ...   \n",
            "684                  0                  7              1  ...   \n",
            "685                  1                  2              7  ...   \n",
            "686                 12                  9            103  ...   \n",
            "\n",
            "     Combined_Sentiment_Pos  Combined_Sentiment_Neg  index        Open  \\\n",
            "0                139.998600               75.999240    562  106.199997   \n",
            "1                 67.999320               11.999880    583  118.800003   \n",
            "2                 13.999860               20.999790    533   98.650002   \n",
            "3                 25.999740               27.999720    748  254.000000   \n",
            "4                 77.199770               64.599690    801  258.000000   \n",
            "..                      ...                     ...    ...         ...   \n",
            "682               27.199999               14.599999    822  273.700012   \n",
            "683               34.399998               17.799999    706  176.500000   \n",
            "684               16.999999                5.600000    672  195.850006   \n",
            "685               23.600000               12.600000    535  103.599998   \n",
            "686               43.799998               27.199998    832  288.250000   \n",
            "\n",
            "           High         Low       Close     Volume  Dividends  Stock Splits  \n",
            "0    108.599998  105.699997  107.699997   48282252        0.0           0.0  \n",
            "1    120.699997  116.150002  118.550003  225208168        0.0           0.0  \n",
            "2    100.099998   98.199997   99.449997   45226055        0.0           0.0  \n",
            "3    265.549988  249.000000  256.290009  156106218        0.0           0.0  \n",
            "4    267.000000  254.500000  265.700012   88316113        0.0           0.0  \n",
            "..          ...         ...         ...        ...        ...           ...  \n",
            "682  275.489990  262.109985  266.809998   57742968        0.0           0.0  \n",
            "683  176.500000  146.300003  172.000000   75272859        0.0           0.0  \n",
            "684  199.699997  190.500000  192.100006   56543869        0.0           0.0  \n",
            "685  105.000000  101.650002  102.150002   69476642        0.0           0.0  \n",
            "686  304.649994  286.850006  299.350006  102148528        0.0           0.0  \n",
            "\n",
            "[687 rows x 30 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the two CSV files\n",
        "merged_data = pd.read_csv('merged_data_all.csv')\n",
        "general_data = pd.read_csv('general.csv')\n",
        "\n",
        "# Concatenate the two dataframes\n",
        "combined_data = pd.concat([merged_data, general_data], ignore_index=True)\n",
        "\n",
        "# Save the combined data to a new CSV file\n",
        "combined_data.to_csv('merged_data_all.csv', index=False)"
      ],
      "metadata": {
        "id": "p2jyHQTQpFap"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}