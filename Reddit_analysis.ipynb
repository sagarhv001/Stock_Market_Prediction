{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "thAL50MXayKq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "from datetime import datetime\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rolling_window = 5\n",
        "commentDF = pd.read_csv('/content/comment_analysis_Indian_Stock_Market_NSE.csv')\n",
        "commentDF['Rolling_Vader_Pos'] = commentDF['Vader Pos'].rolling(window=rolling_window).mean()\n",
        "commentDF['Rolling_Vader_Neg'] = commentDF['Vader Neg'].rolling(window=rolling_window).mean()\n",
        "commentDF['Rolling_TextBlob_Pos'] = commentDF['textblob Positive'].rolling(window=rolling_window).mean()\n",
        "commentDF['Rolling_TextBlob_Neg'] = commentDF['textblob Negative'].rolling(window=rolling_window).mean()\n",
        "\n",
        "# Fill NA values resulting from rolling calculation\n",
        "commentDF.fillna(0, inplace=True)"
      ],
      "metadata": {
        "id": "px70PO83hkDy"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "commentDF['Weighted_Vader_Pos'] = commentDF['Vader Pos'] * commentDF['NumberOfTickerMentions']\n",
        "commentDF['Weighted_Vader_Neg'] = commentDF['Vader Neg'] * commentDF['NumberOfTickerMentions']\n",
        "commentDF['Weighted_TextBlob_Pos'] = commentDF['textblob Positive'] * commentDF['NumberOfTickerMentions']\n",
        "commentDF['Weighted_TextBlob_Neg'] = commentDF['textblob Negative'] * commentDF['NumberOfTickerMentions']\n",
        "\n",
        "# Optional: Normalize by dividing by the total mentions in the same window\n",
        "commentDF['Normalized_Weighted_Vader_Pos'] = commentDF['Weighted_Vader_Pos'] / (commentDF['NumberOfTickerMentions'] + 1e-5)\n",
        "commentDF['Normalized_Weighted_Vader_Neg'] = commentDF['Weighted_Vader_Neg'] / (commentDF['NumberOfTickerMentions'] + 1e-5)"
      ],
      "metadata": {
        "id": "d1NDV3B4-b83"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "commentDF['Combined_Sentiment_Pos'] = commentDF['Rolling_Vader_Pos'] + commentDF['Normalized_Weighted_Vader_Pos']\n",
        "commentDF['Combined_Sentiment_Neg'] = commentDF['Rolling_Vader_Neg'] + commentDF['Normalized_Weighted_Vader_Neg']\n"
      ],
      "metadata": {
        "id": "VU9eJPaM-e8i"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "commentDF.to_csv('comment_analysis_with_sentiments.csv', index=False)"
      ],
      "metadata": {
        "id": "buSrKnKu-hv-"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comment_df = pd.read_csv('general.csv')\n",
        "stock_df = pd.read_csv('/content/stockhistory_^NSEI.csv')"
      ],
      "metadata": {
        "id": "45mYlTaf-lgK"
      },
      "execution_count": 288,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Comment Data Columns:\", comment_df.columns)\n",
        "print(\"Stock Data Columns:\", stock_df.columns)\n",
        "comment_df['Date'] = pd.to_datetime(comment_df['Date'], errors='coerce')\n",
        "stock_df['Date'] = pd.to_datetime(stock_df['Date'], errors='coerce')\n",
        "print(\"Missing values in comment_df 'Date':\", comment_df['Date'].isnull().sum())\n",
        "print(\"Missing values in stock_df 'Date':\", stock_df['Date'].isnull().sum())\n",
        "stock_df = stock_df.dropna(subset=['Date'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERtuRDhG-o5T",
        "outputId": "24a43899-c955-44fd-d1d9-30991767fac7"
      },
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comment Data Columns: Index(['Title', 'Ticker', 'Date', 'NumberOfTickerMentions', 'Vader Neg',\n",
            "       'Vader Pos', 'Vader Neut', 'textblob Negative', 'textblob Positive',\n",
            "       'textblob Neut', 'Rolling_Vader_Pos', 'Rolling_Vader_Neg',\n",
            "       'Rolling_TextBlob_Pos', 'Rolling_TextBlob_Neg', 'Weighted_Vader_Pos',\n",
            "       'Weighted_Vader_Neg', 'Weighted_TextBlob_Pos', 'Weighted_TextBlob_Neg',\n",
            "       'Normalized_Weighted_Vader_Pos', 'Normalized_Weighted_Vader_Neg',\n",
            "       'Combined_Sentiment_Pos', 'Combined_Sentiment_Neg'],\n",
            "      dtype='object')\n",
            "Stock Data Columns: Index(['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Dividends',\n",
            "       'Stock Splits'],\n",
            "      dtype='object')\n",
            "Missing values in comment_df 'Date': 0\n",
            "Missing values in stock_df 'Date': 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "invalid_dates = comment_df[~comment_df['Date'].apply(pd.to_datetime, errors='coerce').notna()]\n",
        "print(invalid_dates)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W581c6ww-r7N",
        "outputId": "a851dd65-9818-4950-800c-c2ce3a248fde"
      },
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [Title, Ticker, Date, NumberOfTickerMentions, Vader Neg, Vader Pos, Vader Neut, textblob Negative, textblob Positive, textblob Neut, Rolling_Vader_Pos, Rolling_Vader_Neg, Rolling_TextBlob_Pos, Rolling_TextBlob_Neg, Weighted_Vader_Pos, Weighted_Vader_Neg, Weighted_TextBlob_Pos, Weighted_TextBlob_Neg, Normalized_Weighted_Vader_Pos, Normalized_Weighted_Vader_Neg, Combined_Sentiment_Pos, Combined_Sentiment_Neg]\n",
            "Index: []\n",
            "\n",
            "[0 rows x 22 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define function to check if the comment contains the stock ticker\n",
        "def contains_ticker(comment, ticker_keywords):\n",
        "    # Regex to match ticker symbol in the comment based on ticker_keywords\n",
        "    pattern = rf\"\\b(?:{'|'.join(ticker_keywords)})\\b\"\n",
        "    return bool(re.search(pattern, comment, re.IGNORECASE))\n",
        "\n",
        "# Filter comments by the ticker keywords\n",
        "def filter_comments_by_ticker(comment_df, ticker_keywords):\n",
        "    # Use boolean indexing to filter the DataFrame directly for keywords\n",
        "    relevant_comments = comment_df[comment_df['Title'].str.contains(rf\"\\b(?:{'|'.join(ticker_keywords)})\\b\", case=False, na=False)]\n",
        "    return relevant_comments\n",
        "\n",
        "# Merge the comment dataframe with stock data based on Date\n",
        "def merge_sentiment_and_stock_data(comment_df, stock_df):\n",
        "    # Ensure 'Date' columns in both dataframes are datetime objects\n",
        "    # Convert Date columns to datetime in both dataframes\n",
        "    comment_df['Date'] = pd.to_datetime(comment_df['Date'], errors='coerce').dt.tz_localize(None)\n",
        "    stock_df = stock_df.reset_index()\n",
        "    stock_df['Date'] = pd.to_datetime(stock_df['Date'], errors='coerce').dt.tz_localize(None)\n",
        "    # Merge filtered comments with stock price data (using 'Date')\n",
        "    final_df = pd.merge(comment_df, stock_df, on='Date', how='inner')\n",
        "    return final_df\n",
        "\n",
        "# Function to process and merge the data\n",
        "def process_data():\n",
        "    # Define the stock ticker and keywords you're interested in\n",
        "    ticker = 'ZOMATO.NS'  # Replace with the correct ticker format\n",
        "    ticker_keywords = ['ZOMATO']  # List of keywords to match in comments\n",
        "\n",
        "    # Load your comment data\n",
        "    comment_df = pd.read_csv('comment_analysis_with_sentiments.csv')\n",
        "\n",
        "    # Load stock price data (e.g., from Yahoo Finance for ticker 'ADANIENT.NSE')\n",
        "    # Get stock data for the last 6 months\n",
        "\n",
        "    if stock_df.empty:\n",
        "        print(f\"No stock data available for {ticker}. Please check the ticker symbol or try another period.\")\n",
        "        return\n",
        "\n",
        "    # Filter comments by the target ticker keywords\n",
        "    relevant_comment_df = filter_comments_by_ticker(comment_df, ticker_keywords)\n",
        "\n",
        "    # Check if relevant_comment_df is empty\n",
        "    if relevant_comment_df.empty:\n",
        "        print(f\"No comments found for ticker: {ticker}\")\n",
        "        return\n",
        "\n",
        "    # Merge the filtered comment data with stock price data based on the 'Date' column\n",
        "    final_df = merge_sentiment_and_stock_data(relevant_comment_df, stock_df)\n",
        "\n",
        "    # Save the merged dataframe to a CSV file\n",
        "    final_df.to_csv(f'merged_data_{ticker}.csv', index=False)\n",
        "\n",
        "    # Display the merged dataframe\n",
        "    print(final_df)\n",
        "\n",
        "# Run the process_data function\n",
        "process_data()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEVEPKuH-vGg",
        "outputId": "448d2408-0899-4519-aaf2-ff482f491419"
      },
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               Title Ticker       Date  \\\n",
            "0                          Zomato doubled my money ðŸ¥¹    NSE 2023-11-08   \n",
            "1             Zomato- Blinkit - Things don't add up.    NSE 2024-09-27   \n",
            "2                             Zomato Investors today    NSE 2022-08-02   \n",
            "3  Why market fell today? Why is Zomato falling? ...    NSE 2024-05-27   \n",
            "4  Zomato, swiggy hike platform fee by 20% to Rs ...    NSE 2024-07-15   \n",
            "5       What do you think will happen to Zomato now?    NSE 2022-08-22   \n",
            "6  Sell shovels during a Gold rush. Pudumjee Pape...    NSE 2024-12-02   \n",
            "\n",
            "   NumberOfTickerMentions  Vader Neg  Vader Pos  Vader Neut  \\\n",
            "0                       1         12         39          47   \n",
            "1                      32          9         21          33   \n",
            "2                      47          8          8          22   \n",
            "3                      68          5         15          34   \n",
            "4                      74         16         38          54   \n",
            "5                      81         14         14          44   \n",
            "6                      81          3         15          15   \n",
            "\n",
            "   textblob Negative  textblob Positive  textblob Neut  ...  \\\n",
            "0                 15                 36             47  ...   \n",
            "1                 23                 29             11  ...   \n",
            "2                  8                 15             15  ...   \n",
            "3                  9                 24             21  ...   \n",
            "4                 18                 54             36  ...   \n",
            "5                 19                 21             32  ...   \n",
            "6                  6                 15             12  ...   \n",
            "\n",
            "   Combined_Sentiment_Pos  Combined_Sentiment_Neg  index        Open  \\\n",
            "0               82.599610               27.399880    569  122.000000   \n",
            "1               38.999993               15.399997    786  275.399994   \n",
            "2               36.799998               24.599998    256   50.000000   \n",
            "3               32.999998               10.599999    700  183.899994   \n",
            "4               61.599995               27.399998    734  225.000000   \n",
            "5               37.999998               26.999998    268   61.000000   \n",
            "6               23.399998                8.600000    829  281.899994   \n",
            "\n",
            "         High         Low       Close     Volume  Dividends  Stock Splits  \n",
            "0  125.699997  121.300003  125.150002   75041688        0.0           0.0  \n",
            "1  286.899994  273.500000  278.149994   58812631        0.0           0.0  \n",
            "2   55.549999   48.400002   55.549999  528899748        0.0           0.0  \n",
            "3  185.449997  178.399994  183.649994   47652253        0.0           0.0  \n",
            "4  232.000000  225.000000  229.149994   51203051        0.0           0.0  \n",
            "5   62.849998   59.250000   62.000000  226773007        0.0           0.0  \n",
            "6  285.700012  280.450012  282.500000   35170688        0.0           0.0  \n",
            "\n",
            "[7 rows x 30 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-274-1e1f86cde112>:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  comment_df['Date'] = pd.to_datetime(comment_df['Date'], errors='coerce').dt.tz_localize(None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f9UrnH3Y_2e2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "\n",
        "def merge_all_csvs(output_file, sort_by=\"Date\"):\n",
        "    \"\"\"\n",
        "    Merges all CSV files matching 'merged_data_*.csv' into a single file and sorts by date.\n",
        "\n",
        "    Args:\n",
        "        output_file (str): Path to save the final merged and sorted CSV file.\n",
        "        sort_by (str): Column to sort by. Default is 'Date'.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Get all merged_data_*.csv files\n",
        "        csv_files = glob.glob(\"/content/merged_data_*.csv\")\n",
        "\n",
        "        if not csv_files:\n",
        "            print(\"No CSV files found matching 'merged_data_*.csv'\")\n",
        "            return\n",
        "\n",
        "        # Read and combine all CSV files\n",
        "        combined_df = pd.concat([pd.read_csv(file) for file in csv_files], ignore_index=True)\n",
        "\n",
        "        # Ensure the sort column is in datetime format if it's a date\n",
        "        if sort_by in combined_df.columns:\n",
        "            combined_df[sort_by] = pd.to_datetime(combined_df[sort_by], errors='coerce')\n",
        "\n",
        "        # Sort the combined DataFrame by the Date column\n",
        "        sorted_df = combined_df.sort_values(by=sort_by)\n",
        "        sorted_df.drop_duplicates(inplace=True)\n",
        "\n",
        "        # Save the merged and sorted DataFrame to a new CSV\n",
        "        sorted_df.to_csv(output_file, index=False)\n",
        "        print(f\"All CSVs have been merged and saved to: {output_file}\")\n",
        "        print(sorted_df.head())  # Print first few rows for verification\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "# Example usage\n",
        "merge_all_csvs(\"merged_data_all.csv\", sort_by=\"Date\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnMsiz0_GhO2",
        "outputId": "23ac4707-4dcd-4670-9035-c0cc98bd5c2e"
      },
      "execution_count": 296,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All CSVs have been merged and saved to: merged_data_all.csv\n",
            "                                   Title Ticker       Date  \\\n",
            "475                Meme of the day 12/02    NSE 2021-02-12   \n",
            "300                              Stonks!    NSE 2021-04-09   \n",
            "49   Daily Story but today was different    NSE 2021-04-12   \n",
            "185       Shared by a friend. Relate max    NSE 2021-05-10   \n",
            "357                   Bitcoiners to Elon    NSE 2021-05-17   \n",
            "\n",
            "    NumberOfTickerMentions Vader Neg Vader Pos Vader Neut textblob Negative  \\\n",
            "475                     68         0        15         15                 1   \n",
            "300                     59         0         0          5                 0   \n",
            "49                       5         5         3         14                 7   \n",
            "185                     36         3         4         11                 3   \n",
            "357                     60         1         3          8                 3   \n",
            "\n",
            "    textblob Positive textblob Neut  ...  Combined_Sentiment_Pos  \\\n",
            "475                11            18  ...               34.399998   \n",
            "300                 0             5  ...               45.400000   \n",
            "49                  5            10  ...               45.799994   \n",
            "185                 5            10  ...               23.999999   \n",
            "357                 5             4  ...               28.400000   \n",
            "\n",
            "     Combined_Sentiment_Neg  index          Open          High           Low  \\\n",
            "475                6.800000   3280  15186.200195  15243.500000  15081.000000   \n",
            "300               16.400000   3317  14882.650391  14918.450195  14785.650391   \n",
            "49                29.599990   3318  14644.650391  14652.500000  14248.700195   \n",
            "185               10.599999   3336  14928.250000  14966.900391  14892.500000   \n",
            "357                5.000000   3340  14756.250000  14938.000000  14725.349609   \n",
            "\n",
            "            Close  Volume  Dividends  Stock Splits  \n",
            "475  15163.299805  571800        0.0           0.0  \n",
            "300  14834.849609  504100        0.0           0.0  \n",
            "49   14310.799805  650200        0.0           0.0  \n",
            "185  14942.349609  522000        0.0           0.0  \n",
            "357  14923.150391  546400        0.0           0.0  \n",
            "\n",
            "[5 rows x 30 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-296-1b717b74e67a>:22: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  combined_df = pd.concat([pd.read_csv(file) for file in csv_files], ignore_index=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the merged data\n",
        "merged_data = pd.read_csv('merged_data_all.csv')\n",
        "\n",
        "# Load the original comment data\n",
        "comment_data = pd.read_csv('comment_analysis_with_sentiments.csv')\n",
        "\n",
        "# Find comments not present in the merged data\n",
        "# Assuming 'Title' is the unique identifier in both DataFrames\n",
        "comments_not_in_merged = comment_data[~comment_data['Title'].isin(merged_data['Title'])]\n",
        "\n",
        "# Save the comments not in merged data to a new CSV file\n",
        "comments_not_in_merged.to_csv('general.csv', index=False)"
      ],
      "metadata": {
        "id": "3_KdM493BI4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For the General Comments\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from datetime import datetime\n",
        "\n",
        "# Merge the comment dataframe with stock data based on Date\n",
        "def merge_sentiment_and_stock_data(comment_df, stock_df):\n",
        "    # Ensure 'Date' columns in both dataframes are datetime objects\n",
        "    # Convert Date columns to datetime in both dataframes\n",
        "    comment_df['Date'] = pd.to_datetime(comment_df['Date'], errors='coerce').dt.tz_localize(None)\n",
        "    stock_df = stock_df.reset_index()\n",
        "    stock_df['Date'] = pd.to_datetime(stock_df['Date'], errors='coerce').dt.tz_localize(None)\n",
        "    # Merge filtered comments with stock price data (using 'Date')\n",
        "    final_df = pd.merge(comment_df, stock_df, on='Date', how='inner')\n",
        "    return final_df\n",
        "\n",
        "# Function to process and merge the data\n",
        "def process_data():\n",
        "    # Define the stock ticker for NIFTY 50\n",
        "    ticker = '^NSEI'  # Ticker for NIFTY 50\n",
        "\n",
        "    # Load your comment data\n",
        "    comment_df = pd.read_csv('general.csv')\n",
        "\n",
        "    # Fetch stock price data from Yahoo Finance\n",
        "    # Last 1 year of data\n",
        "\n",
        "    # Check if stock data is empty\n",
        "    if stock_df.empty:\n",
        "        print(f\"No stock data available for {ticker}. Please check the ticker symbol or try another period.\")\n",
        "        return\n",
        "\n",
        "    # Merge the comment data with stock price data based on the 'Date' column\n",
        "    final_df = merge_sentiment_and_stock_data(comment_df, stock_df)\n",
        "\n",
        "    # Save the merged dataframe to a CSV file\n",
        "    output_file = f'merged_data_{ticker}.csv'\n",
        "    final_df.to_csv(output_file, index=False)\n",
        "\n",
        "    # Display the merged dataframe\n",
        "    print(f\"Merged data saved to {output_file}\")\n",
        "    print(final_df)\n",
        "\n",
        "# Run the process_data function\n",
        "process_data()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "DbOYa-tXJMhA",
        "outputId": "06376350-9a13-40e5-ea0b-366355452b06"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'general.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a6f85b71e17c>\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# Run the process_data function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-a6f85b71e17c>\u001b[0m in \u001b[0;36mprocess_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Load your comment data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mcomment_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'general.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Fetch stock price data from Yahoo Finance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'general.csv'"
          ]
        }
      ]
    }
  ]
}